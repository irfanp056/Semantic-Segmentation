{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm \nfrom PIL import Image\nimport os\nimport warnings\n\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nsns.set_style('darkgrid')\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:39:55.999294Z","iopub.execute_input":"2023-09-28T13:39:55.999783Z","iopub.status.idle":"2023-09-28T13:40:01.865538Z","shell.execute_reply.started":"2023-09-28T13:39:55.999694Z","shell.execute_reply":"2023-09-28T13:40:01.864452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import labeling key from GitHub\n\nThe following code below is found in the following GitHub repository: https://github.molgen.mpg.de/mohomran/cityscapes/blob/master/scripts/helpers/labels.py#L55\n\nThis repository helper function was created by one of the team members of the CityScapes dataset","metadata":{}},{"cell_type":"code","source":"from collections import namedtuple\n\n#--------------------------------------------------------------------------------\n# Definitions\n#--------------------------------------------------------------------------------\n\n# a label and all meta information\nLabel = namedtuple( 'Label' , [\n\n    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n                    # We use them to uniquely name a class\n\n    'id'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n\n    'trainId'     , # An integer ID that overwrites the ID above, when creating ground truth\n                    # images for training.\n                    # For training, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n\n    'category'    , # The name of the category that this label belongs to\n\n    'categoryId'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    'hasInstances', # Whether this label distinguishes between single instances or not\n\n    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    'color'       , # The color of this label\n    ] )\n\n\n#--------------------------------------------------------------------------------\n# A list of all labels\n#--------------------------------------------------------------------------------\n\n# Please adapt the train IDs as appropriate for you approach.\n# Note that you might want to ignore labels with ID 255 during training.\n# Make sure to provide your results using the original IDs and not the training IDs.\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  'road'                 ,  7 ,        0 , 'ground'          , 1       , False        , False        , (128, 64,128) ),\n    Label(  'sidewalk'             ,  8 ,        1 , 'ground'          , 1       , False        , False        , (244, 35,232) ),\n    Label(  'parking'              ,  9 ,      255 , 'ground'          , 1       , False        , True         , (250,170,160) ),\n    Label(  'rail track'           , 10 ,      255 , 'ground'          , 1       , False        , True         , (230,150,140) ),\n    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  'license plate'        , 34 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:40:01.867514Z","iopub.execute_input":"2023-09-28T13:40:01.868088Z","iopub.status.idle":"2023-09-28T13:40:01.898412Z","shell.execute_reply.started":"2023-09-28T13:40:01.868059Z","shell.execute_reply":"2023-09-28T13:40:01.897267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_FILTERS = 64\nKERNEL_SIZE = 3\nN_CLASSES = len(labels)\nIMAGE_SIZE = [128, 128]\nIMAGE_SHAPE = IMAGE_SIZE + [3,]\n\nEPOCHS = 3\nBATCH_SIZE = 16\nMODEL_CHECKPOINT_FILEPATH = './cityscapes-unet.ckpt'\n\nid2color = { label.id : np.asarray(label.color) for label in labels }","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:40:01.900126Z","iopub.execute_input":"2023-09-28T13:40:01.900591Z","iopub.status.idle":"2023-09-28T13:40:01.913612Z","shell.execute_reply.started":"2023-09-28T13:40:01.900554Z","shell.execute_reply":"2023-09-28T13:40:01.912744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------------------------------------------------------------------------\n#  Load images in, crop for the image and mask, resize, and then encode mask\n#--------------------------------------------------------------------------------\n\ndef image_mask_split(filename, image_size):\n    image_mask = Image.open(filename)\n    \n    image, mask = image_mask.crop([0, 0, 256, 256]), image_mask.crop([256, 0, 512, 256])\n    image = image.resize(image_size)\n    mask = mask.resize(image_size)\n\n    image = np.array(image) / 255 # crop image section and reformat as normalized np array\n    mask = np.array(mask) # crop mask section and reformat as np array\n    \n    return image, mask\n\n#--------------------------------------------------------------------------------\n# Remap mask half of image into sparse matrix using closest color value\n#--------------------------------------------------------------------------------\n\ndef find_closest_labels_vectorized(mask, mapping): # 'mapping' is a RGB color tuple to categorical number dictionary\n    \n    closest_distance = np.full([mask.shape[0], mask.shape[1]], 10000) \n    closest_category = np.full([mask.shape[0], mask.shape[1]], None)   \n\n    for id, color in mapping.items(): # iterate over every color mapping\n        dist = np.sqrt(np.linalg.norm(mask - color.reshape([1,1,-1]), axis=-1))\n        is_closer = closest_distance > dist\n        closest_distance = np.where(is_closer, dist, closest_distance)\n        closest_category = np.where(is_closer, id, closest_category)\n    \n    return closest_category","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:40:01.917975Z","iopub.execute_input":"2023-09-28T13:40:01.918272Z","iopub.status.idle":"2023-09-28T13:40:01.931070Z","shell.execute_reply.started":"2023-09-28T13:40:01.918235Z","shell.execute_reply":"2023-09-28T13:40:01.930130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets\nThe images will be loaded in using the functions defined above. The mask half of the image will need to be encoded into usable categorical values.\n\nOne issue that arose when first attempting to encode is that the image is not cleanly segmented into section.  The boundaries between categories have intermediary values.  This is most likely an artifact from anti-aliasing when the images were being resized into 256 x 512 pixels. Thus the encoding performs a loop through every categorical variables color and finds the one that is has the closest vector norm.","metadata":{}},{"cell_type":"code","source":"train_filepath = '/kaggle/input/cityscapes-image-pairs/cityscapes_data/train/'\nval_filepath = '/kaggle/input/cityscapes-image-pairs/cityscapes_data/val/'\n\n# Store the images, the masks, and the encoded masks\ntrain_images = [] \ntrain_masks = []\ntrain_masks_enc = []\nval_images = []\nval_masks = []\nval_masks_enc = []\n\nfor train_file in tqdm(os.listdir(train_filepath), desc = 'Building Training Dataset: '):\n    image, mask = image_mask_split(train_filepath + train_file, IMAGE_SIZE)\n    train_images.append(image)\n    train_masks.append(mask)\n    train_masks_enc.append(find_closest_labels_vectorized(mask, id2color))\n    \nfor val_file in tqdm(os.listdir(val_filepath), desc = 'Building Validation Dataset: '):\n    image, mask = image_mask_split(val_filepath + val_file, IMAGE_SIZE)\n    val_images.append(image)\n    val_masks.append(mask)\n    val_masks_enc.append(find_closest_labels_vectorized(mask, id2color))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:40:01.934487Z","iopub.execute_input":"2023-09-28T13:40:01.934865Z","iopub.status.idle":"2023-09-28T13:42:57.011569Z","shell.execute_reply.started":"2023-09-28T13:40:01.934827Z","shell.execute_reply":"2023-09-28T13:42:57.010412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the image, mask, and encoded mask\n\nLets take a look at a couple of the images and how well the encoding did.\n\nAs stated before, there is an issue with boundary edges and this is quite evident in the encoding. However, it still looks to have done a good job overall with the vast majority of each image being encoded correctly.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[20, 14])\n\nfor i in range(2):\n    img = train_images[i]\n    msk = train_masks[i]\n    enc = train_masks_enc[i]\n    tmp = np.zeros([enc.shape[0], enc.shape[1], 3])\n    \n    for row in range(enc.shape[0]):\n        for col in range(enc.shape[1]):\n            tmp[row, col, :] = id2color[enc[row, col]]\n            tmp = tmp.astype('uint8')\n            \n    plt.subplot(2, 3, i*3 + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.gca().set_title('Sample Image {}'.format(str(i+1)))\n    \n    plt.subplot(2, 3, i*3 + 2)\n    plt.imshow(msk)\n    plt.axis('off')\n    plt.gca().set_title('Sample Mask {}'.format(str(i+1)))\n    \n    plt.subplot(2, 3, i*3 + 3)\n    plt.imshow(tmp)\n    plt.axis('off')\n    plt.gca().set_title('Sample Encoded Mask {}'.format(str(i+1)))\n    \nplt.subplots_adjust(wspace=0, hspace=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:42:57.013159Z","iopub.execute_input":"2023-09-28T13:42:57.014792Z","iopub.status.idle":"2023-09-28T13:42:58.385376Z","shell.execute_reply.started":"2023-09-28T13:42:57.014747Z","shell.execute_reply":"2023-09-28T13:42:58.384509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete the masks as they are no longer needed to free up RAM\ndel train_masks, val_masks","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:42:58.386417Z","iopub.execute_input":"2023-09-28T13:42:58.386761Z","iopub.status.idle":"2023-09-28T13:42:58.396729Z","shell.execute_reply.started":"2023-09-28T13:42:58.386728Z","shell.execute_reply":"2023-09-28T13:42:58.394682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = np.stack(train_images).astype('float32')\ntrain_masks_enc = np.stack(train_masks_enc).astype('float32')\n\nval_images = np.stack(val_images).astype('float32')\nval_masks_enc = np.stack(val_masks_enc).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:42:58.399459Z","iopub.execute_input":"2023-09-28T13:42:58.400092Z","iopub.status.idle":"2023-09-28T13:43:04.699844Z","shell.execute_reply.started":"2023-09-28T13:42:58.400057Z","shell.execute_reply":"2023-09-28T13:43:04.698740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the U-Net Model\n\nThis functions to build this U-Net model are highly inspired by Andrew Ng's Deep Learning Specialization\n\nThere is a defined function for the downscaling convolution block and a function for the upsampling block","metadata":{"execution":{"iopub.status.busy":"2022-11-26T20:03:06.47008Z","iopub.execute_input":"2022-11-26T20:03:06.471302Z","iopub.status.idle":"2022-11-26T20:03:06.475379Z","shell.execute_reply.started":"2022-11-26T20:03:06.471259Z","shell.execute_reply":"2022-11-26T20:03:06.474586Z"}}},{"cell_type":"code","source":"def conv_block(inputs=None, n_filters=32, kernel_size = 3, dropout_prob = 0, max_pooling=True):\n    \n    conv = Conv2D(n_filters, # Number of filters\n                  kernel_size = 3, # Kernel size   \n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(inputs)\n    \n    conv = Conv2D(n_filters, # Number of filters\n                  kernel_size = 3,   # Kernel size\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv)\n    \n    # if dropout_prob > 0 add a dropout layer, with the variable dropout_prob as parameter\n    if dropout_prob > 0:\n        conv = Dropout(dropout_prob)(conv)\n        \n    # if max_pooling is True add a MaxPooling2D with 2x2 pool_size\n    if max_pooling:\n        next_layer = MaxPooling2D(pool_size = (2,2))(conv)\n    else:\n        next_layer = conv\n        \n    skip_connection = conv\n    \n    return next_layer, skip_connection\n\ndef upsampling_block(expansive_input, contractive_input, n_filters=32, kernel_size = 3):\n    \n    up = Conv2DTranspose(\n                 n_filters,    # number of filters\n                 kernel_size = kernel_size,    # Kernel size\n                 strides = (2,2),\n                 padding = 'same')(expansive_input)\n    \n    # Merge the previous output and the contractive_input\n    merge = concatenate([up, contractive_input], axis=3)\n    \n    conv = Conv2D(n_filters,   # Number of filters\n                 kernel_size = (3,3),     # Kernel size\n                 activation='relu',\n                 padding='same',\n                 kernel_initializer='he_normal')(merge)\n    conv = Conv2D(n_filters,  # Number of filters\n                 kernel_size = (3,3),   # Kernel size\n                 activation='relu',\n                 padding='same',\n                 kernel_initializer='he_normal')(conv)\n    \n    return conv","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:43:04.701670Z","iopub.execute_input":"2023-09-28T13:43:04.702084Z","iopub.status.idle":"2023-09-28T13:43:04.714507Z","shell.execute_reply.started":"2023-09-28T13:43:04.702042Z","shell.execute_reply":"2023-09-28T13:43:04.713318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_unet_model(image_shape, n_filters, kernel_size, n_classes):\n\n    inputs = Input(image_shape)\n\n    # Contracting Path (encoding)\n    cblock1 = conv_block(inputs, n_filters, kernel_size)\n    cblock2 = conv_block(cblock1[0], n_filters * 2, kernel_size)\n    cblock3 = conv_block(cblock2[0], n_filters * 4, kernel_size, dropout_prob = 0.3)\n    cblock4 = conv_block(cblock3[0], n_filters * 8, kernel_size, dropout_prob = 0.3) # Include a dropout_prob of 0.3 for this layer\n    cblock5 = conv_block(cblock4[0], n_filters * 16, kernel_size, dropout_prob = 0.3, max_pooling=False) \n\n    # Expanding Path (decoding)\n    # Add the first upsampling_block.\n\n    ublock6 = upsampling_block(cblock5[0], cblock4[1], n_filters * 8, kernel_size)\n    ublock7 = upsampling_block(ublock6, cblock3[1], n_filters * 4, kernel_size)\n    ublock8 = upsampling_block(ublock7, cblock2[1], n_filters * 2, kernel_size)\n    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters, kernel_size)\n\n    conv9 = Conv2D(n_filters,\n                 kernel_size = kernel_size,\n                 activation='relu',\n                 padding='same',\n                 kernel_initializer='he_normal')(ublock9)\n\n    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\n    conv10 = Conv2D(n_classes, kernel_size = 1, padding='same')(conv9)\n\n    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:43:04.718960Z","iopub.execute_input":"2023-09-28T13:43:04.719321Z","iopub.status.idle":"2023-09-28T13:43:04.732787Z","shell.execute_reply.started":"2023-09-28T13:43:04.719285Z","shell.execute_reply":"2023-09-28T13:43:04.731553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the functions to build the model and display it below\nmodel = create_unet_model(IMAGE_SHAPE, N_FILTERS, KERNEL_SIZE, N_CLASSES)\n\ntf.keras.utils.plot_model(model, show_shapes = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:43:04.734547Z","iopub.execute_input":"2023-09-28T13:43:04.735261Z","iopub.status.idle":"2023-09-28T13:43:08.904690Z","shell.execute_reply.started":"2023-09-28T13:43:04.735223Z","shell.execute_reply":"2023-09-28T13:43:08.903560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n\nThe model checkpoint callback will be used to save only the best epoch.\n\nThere is no early stopping as I want to visually see whether the model begins to overfit and to what extent.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer = 'adam',\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics = ['accuracy'])\n\nmodel_checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_FILEPATH,\n                                   monitor='val_accuracy',\n                                   save_best_only=True,\n                                   save_weights_only=True,\n                                   verbose=1,\n                                   mode = 'max')\n\ncallbacks = [model_checkpoint]\n\nhistory = model.fit(x = train_images,\n                    y = train_masks_enc,\n                    batch_size = BATCH_SIZE,\n                    epochs = EPOCHS,\n                    validation_data = (val_images, val_masks_enc),\n                    callbacks = callbacks)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:43:08.907184Z","iopub.execute_input":"2023-09-28T13:43:08.907892Z","iopub.status.idle":"2023-09-28T13:44:42.108673Z","shell.execute_reply.started":"2023-09-28T13:43:08.907846Z","shell.execute_reply":"2023-09-28T13:44:42.107562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,6))\ntitle_fontsize = 16\naxis_fontsize = 12\n\nax1.plot(range(1, EPOCHS + 1), history.history['loss'], marker='o', label='Training loss')\nax1.plot(range(1, EPOCHS + 1), history.history['val_loss'], marker='o', label='Validation Loss')\nax1.legend()\nax1.set_xticks(range(1, EPOCHS + 1))\nax1.set_title('Loss', fontsize=title_fontsize)\nax1.set_xlabel('Epoch', fontsize=axis_fontsize)\n\nax2.plot(range(1, EPOCHS + 1), history.history['accuracy'], marker='o', label='Training Accuracy')\nax2.plot(range(1, EPOCHS + 1), history.history['val_accuracy'], marker='o', label='Validation Accuracy')\nax2.legend()\nax2.set_xticks(range(1, EPOCHS + 1))\nax2.set_title('Accuracy', fontsize=title_fontsize)\nax2.set_xlabel('Epoch', fontsize=axis_fontsize);","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:44:42.111941Z","iopub.execute_input":"2023-09-28T13:44:42.112325Z","iopub.status.idle":"2023-09-28T13:44:42.569405Z","shell.execute_reply.started":"2023-09-28T13:44:42.112284Z","shell.execute_reply":"2023-09-28T13:44:42.568415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"model.load_weights(MODEL_CHECKPOINT_FILEPATH) # load the best model weights\n\nval_loss, val_accuracy = model.evaluate(x = val_images, y = val_masks_enc) # re-evaluate on the validation data\n\nprint('\\n\\033[1m' + 'The model had an accuracy score of {}%!!'.format(round(100*val_accuracy, 2)) + '\\033[0m')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:44:42.571132Z","iopub.execute_input":"2023-09-28T13:44:42.571833Z","iopub.status.idle":"2023-09-28T13:44:53.446061Z","shell.execute_reply.started":"2023-09-28T13:44:42.571793Z","shell.execute_reply":"2023-09-28T13:44:53.444943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see how the model does on some on the validation images.\n\nWe will visually look side-by-side as the true model mask and the predicted model mask on some of the images from the validation set","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[15, 20])\n\nfor i in range(4):    \n    img = val_images[i]\n    enc = val_masks_enc[i]\n    \n    pred = model.predict(img.reshape([1] + IMAGE_SHAPE))\n    pred = np.squeeze(np.argmax(pred, axis=-1))\n    \n    tmp1 = np.zeros([enc.shape[0], enc.shape[1], 3])\n    tmp2 = np.zeros([enc.shape[0], enc.shape[1], 3])\n    \n    \n    for row in range(enc.shape[0]):\n        for col in range(enc.shape[1]):\n            tmp1[row, col, :] = id2color[enc[row, col]]\n            tmp1 = tmp1.astype('uint8')\n                     \n            tmp2[row, col, :] = id2color[pred[row, col]]\n            tmp2 = tmp2.astype('uint8')\n            \n    plt.subplot(4, 3, i*3 + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.gca().set_title('Image {}'.format(str(i+1)))\n    \n    plt.subplot(4, 3, i*3 + 2)\n    plt.imshow(tmp1)\n    plt.axis('off')\n    plt.gca().set_title('Encoded Mask {}'.format(str(i+1)))\n    \n    plt.subplot(4, 3, i*3 + 3)\n    plt.imshow(tmp2)\n    plt.axis('off')\n    plt.gca().set_title('Model Prediction {}'.format(str(i+1)))\n    \nplt.subplots_adjust(wspace=0, hspace=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T13:44:53.447733Z","iopub.execute_input":"2023-09-28T13:44:53.448439Z","iopub.status.idle":"2023-09-28T13:44:56.905054Z","shell.execute_reply.started":"2023-09-28T13:44:53.448398Z","shell.execute_reply":"2023-09-28T13:44:56.903943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you liked this notebook, please leave an upvote!","metadata":{}}]}